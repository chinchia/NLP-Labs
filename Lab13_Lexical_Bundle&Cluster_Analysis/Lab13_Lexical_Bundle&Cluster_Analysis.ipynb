{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 計算語料庫特徵關鍵詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import defaultdict\n",
    "import nltk, pickle\n",
    "import pprint\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def words(text): return re.findall(\"([a-z'-]+|[0-9]+)\", text.lower())\n",
    "\n",
    "count_web1t = [ line.strip().split('\\t') for line in open('count_1w.txt').readlines() ]\n",
    "count_web1t = dict([ (word, int(count)) for word, count in count_web1t ])\n",
    "\n",
    "##########################################################\n",
    "count_how_to = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "##########################################################\n",
    "\n",
    "chapterno = 1\n",
    "for chapter in open('how.to.say.it.(raw).txt').read().split('<chapter>')[1:-1]:\n",
    "    sentences = sent_detector.tokenize(chapter[chapter.index('\\nPHRASES\\n')+len('\\nPHRASES\\n'):])\n",
    "    for sentence in sentences:\n",
    "        for word in words(sentence):\n",
    "            ###################################\n",
    "            count_how_to[chapterno][word] += 1\n",
    "            ###################################\n",
    "    chapterno += 1\n",
    "    \n",
    "def is_key(word, count, total):\n",
    "    if word not in count_web1t: return False\n",
    "    rate = math.log10(count)-math.log10(total)-(math.log10(count_web1t[word])-12)\n",
    "    return rate >= 1\n",
    "\n",
    "#################################\n",
    "keywords = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for i in range(1, len(count_how_to)+1):\n",
    "    total = sum(count_how_to[i].values())\n",
    "    keyword = [ (word, count) for word, count in count_how_to[i].items() if is_key(word, count, total) and count>3]\n",
    "    keyword = sorted(keyword, key=lambda x: -x[1])\n",
    "    for k,v in keyword:\n",
    "        keywords[i][k] = v\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "            {'your': 23,\n",
       "             'we': 19,\n",
       "             'accept': 13,\n",
       "             'me': 12,\n",
       "             'pleasure': 10,\n",
       "             'happy': 10,\n",
       "             'pleased': 9,\n",
       "             'dear': 9,\n",
       "             'm': 7,\n",
       "             's': 6,\n",
       "             'able': 5,\n",
       "             'sincerely': 5,\n",
       "             'forward': 5,\n",
       "             'discuss': 5,\n",
       "             'say': 4,\n",
       "             'thank': 4,\n",
       "             'delighted': 4,\n",
       "             'look': 4,\n",
       "             'offer': 4,\n",
       "             'much': 4,\n",
       "             'invitation': 4,\n",
       "             'thanks': 4,\n",
       "             'enclosed': 4})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 計算各章的詞彙束（關鍵片語）\n",
    "### 　各章關鍵片語的條件，次數出現超過平均值的章節\n",
    "### 　例如 accept 出現在各章的次數 (1, 8), (2, 2), (15, 1)\n",
    "### 　8 > (8+2+1)/3 所以 accept 是第１章的關鍵詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def words(text): return re.findall(\"([a-zA-Z'-]+|[0-9]+)\", text)\n",
    "def ngrams(tokens, n=4): return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n) ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = '''01. Accept; 02. Confirm; 03. Adjust; 04. Advice; 05. Birthday; 06. Announce; 07. Apologize; 08. Apply; 09. Appointment; 10. Appreciate; 11. Late; 12. Collect; 13. Complaint; 14. Congrat; 15. Contract; 16. CoverLetters; 17. Credit; 18. Disagree; 19. ToEditor; 20. E-mail; 21. Employ; 22. Family; 23. Fax; 24. Follow-up; 25. RaiseFund; 26. Get-well; 27. Goodwill; 28. Holiday; 29. Instruct; 30. Introduce; 31. Invite; 32. Love; 33. Memos; 34. ToNeighbor; 35. Order; 36. Club; 37. Query; 38. Refer; 39. Refuse; 40. Report; 41. Request; 42. Respond; 43. Resume; 44. Sales; 45. Sensitive; 46. Sympathy; 47. Thank-you; 48. Travel; 49. Wedding; 50. Welcome'''\n",
    "chapters = [ x.split() for x in chapters.split('; ')]\n",
    "chaptername = dict([ (int(x[:-1]), x+y) for x, y in chapters ])\n",
    "\n",
    "count_chapter = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "##################################################\n",
    "# calculate counts of each chapters based on keys\n",
    "appear_times = []\n",
    "for i in range(1, len(chapters)+1):\n",
    "    appear_time = 0\n",
    "    for j in range(1, len(keywords)+1):\n",
    "        chapt = chapters[i-1][1].lower()\n",
    "        if chapt in keywords[j]:\n",
    "            count_chapter[chapt][j] = keywords[j][chapt]\n",
    "            appear_time += 1\n",
    "        else:\n",
    "            count_chapter[chapt][j] = 0\n",
    "    appear_times.append(appear_time)\n",
    "\n",
    "# calculate total counts of all chapters for each keys\n",
    "chapter_total_counts = defaultdict(lambda: 0)\n",
    "for i in range(1, len(count_chapter)+1):\n",
    "    for k,v in count_chapter.items():\n",
    "        chapter_total_counts[k] += v[i]\n",
    "# normalize to average value        \n",
    "chapterno = 0\n",
    "for k,v in chapter_total_counts.items():\n",
    "    if appear_times[chapterno] != 0:\n",
    "        chapter_total_counts[k] = v / appear_times[chapterno]\n",
    "    chapterno += 1\n",
    "\n",
    "# compare with the average value to decide each chapter's key\n",
    "chapt_key = []\n",
    "for k,v in count_chapter.items():\n",
    "    for chapt, cnts in v.items():\n",
    "        if cnts > chapter_total_counts[k]:\n",
    "            chapt_key.append((chapt, k))\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'accept'),\n",
       " (2, 'confirm'),\n",
       " (10, 'appreciate'),\n",
       " (39, 'appreciate'),\n",
       " (41, 'appreciate'),\n",
       " (47, 'appreciate'),\n",
       " (12, 'credit'),\n",
       " (17, 'credit'),\n",
       " (28, 'family'),\n",
       " (46, 'family'),\n",
       " (5, 'love'),\n",
       " (22, 'love'),\n",
       " (32, 'love'),\n",
       " (35, 'order'),\n",
       " (40, 'report'),\n",
       " (49, 'wedding')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapt_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 做各章關鍵詞的 cluster analysis （利用 Linggle 的 A and B 查詢）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accept', 'credit', 'wedding', 'family', 'love', 'order', 'confirm', 'report', 'appreciate']\n",
      "\n",
      "[['family and love', 30965],\n",
      " ['credit and credit', 29086],\n",
      " ['love and appreciate', 17201],\n",
      " ['love and love', 16494],\n",
      " ['love and family', 15699],\n",
      " ['report and credit', 13485],\n",
      " ['love and accept', 10694],\n",
      " ['family and family', 10223],\n",
      " ['order and credit', 9542],\n",
      " ['accept and love', 5243],\n",
      " ['appreciate and love', 4868],\n",
      " ['accept and appreciate', 4647],\n",
      " ['report and order', 3850],\n",
      " ['order and order', 3665],\n",
      " ['report and report', 2792],\n",
      " ['wedding and wedding', 2650],\n",
      " ['appreciate and accept', 1679],\n",
      " ['order and family', 1618],\n",
      " ['family and wedding', 1522],\n",
      " ['wedding and family', 1391],\n",
      " ['order and confirm', 1101],\n",
      " ['order and report', 844],\n",
      " ['accept and confirm', 780],\n",
      " ['family and order', 744],\n",
      " ['credit and report', 723],\n",
      " ['report and confirm', 577],\n",
      " ['report and accept', 559],\n",
      " ['order and accept', 520],\n",
      " ['love and wedding', 515],\n",
      " ['confirm and report', 402],\n",
      " ['accept and accept', 383],\n",
      " ['wedding and love', 383],\n",
      " ['love and order', 363],\n",
      " ['confirm and accept', 360],\n",
      " ['accept and report', 341],\n",
      " ['order and love', 314],\n",
      " ['accept and credit', 280],\n",
      " ['family and appreciate', 276],\n",
      " ['credit and accept', 260],\n",
      " ['credit and order', 238],\n",
      " ['family and accept', 202],\n",
      " ['family and report', 196],\n",
      " ['credit and family', 182],\n",
      " ['report and family', 152],\n",
      " ['report and appreciate', 116],\n",
      " ['credit and love', 104],\n",
      " ['family and credit', 103],\n",
      " ['wedding and order', 98],\n",
      " ['order and appreciate', 85],\n",
      " ['love and credit', 85]]\n"
     ]
    }
   ],
   "source": [
    "from linggle import Linggle\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "linggle = Linggle()\n",
    "\n",
    "def ngramcount(query):\n",
    "    return linggle[query]\n",
    "\n",
    "###################################################\n",
    "accept_words = list(set([y for x,y in chapt_key]))\n",
    "###################################################\n",
    "print (accept_words)\n",
    "print ()\n",
    "\n",
    "and_grams = ngramcount('%s and %s'%('/'.join(accept_words), '/'.join(accept_words)))\n",
    "pprint.pprint (and_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
