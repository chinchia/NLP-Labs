{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* Code to accompany the chapter \"Natural Language Corpus Data\"\n",
    "from the book \"Beautiful Data\" (Segaran and Hammerbacher, 2009)\n",
    "http://oreilly.com/catalog/9780596157111/\n",
    "\n",
    "Code copyright (c) 2008-2009 by Peter Norvig\n",
    "\n",
    "You are free to use this code under the MIT licencse: \n",
    "http://www.opensource.org/licenses/mit-license.php\n",
    "\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import re, string, random, glob, operator, heapq\n",
    "from collections import defaultdict\n",
    "from math import log10\n",
    "from functools import reduce\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f.\"\n",
    "    table = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in table:\n",
    "            table[args] = f(*args)\n",
    "        return table[args]\n",
    "    fmemo.memo = table\n",
    "    return fmemo\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Segmentation (p. 223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "N = 1024908267229 ## Number of tokens\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    for line in open(name):\n",
    "        yield line.split(sep)\n",
    "\n",
    "def avoid_long_words(key, N):\n",
    "    \"Estimate the probability of an unknown word.\"\n",
    "    return 10./(N * 10**len(key))\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.itervalues()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "\n",
    "        \n",
    "Pw  = Pdist(datafile('data/count_1w.txt'), N, avoid_long_words)\n",
    "\n",
    "#### segment2: second version, with bigram counts, (p. 226-227)\n",
    "\n",
    "def cPw(word, prev):\n",
    "    \"Conditional probability of word, given previous word.\"\n",
    "    try:\n",
    "        return P2w[prev + ' ' + word]/float(Pw[prev])\n",
    "    except KeyError:\n",
    "        return Pw(word)\n",
    "\n",
    "P2w = Pdist(datafile('data/count_2w.txt'), N)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def splits(text, L=20):\n",
    "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "    return [(text[:i+1], text[i+1:]) \n",
    "            for i in range(min(len(text), L))]\n",
    "\n",
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return reduce(operator.mul, nums, 1)\n",
    "\n",
    "def Pwords(words): \n",
    "    \"The Naive Bayes probability of a sequence of words.\"\n",
    "    return product(Pw(w) for w in words)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the best segmentation of text.\"\n",
    "    if not text: return []\n",
    "    candidates = ([first]+segment(rem) for first,rem in splits(text))\n",
    "    return max(candidates, key=Pwords)\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colorless', 'green', 'ideas', 'sleep', 'furiously', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('colorlessgreenideassleepfuriously.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
