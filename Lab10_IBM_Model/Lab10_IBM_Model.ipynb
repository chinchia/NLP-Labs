{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import csv\n",
    "import tqdm\n",
    "from copy import deepcopy\n",
    "from math import log, isclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the tokenized parallel corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_tokens_path = './cambridge_parallel_tokens.tsv'\n",
    "parallel_tokens = []\n",
    "with open(parallel_tokens_path, 'r', encoding='utf8') as f:\n",
    "    for en_tokens, ch_tokens in csv.reader(f, delimiter='\\t'):\n",
    "        parallel_tokens.append([en_tokens.split('#sep#'),\n",
    "                                ch_tokens.split('#sep#')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'ve\", 'bought', 'a', 'car', '.'], ['我', '買', '了', '輛', '汽車', '。']]\n",
      "[['she', \"'s\", 'got', 'a', 'boyfriend', '.'], ['她', '交', '了', '個', '男朋友', '。']]\n",
      "[['there', 'was', 'a', 'sudden', 'loud', 'noise', '.'], ['突然', '發出', '一', '聲', '巨響', '。']]\n"
     ]
    }
   ],
   "source": [
    "for x in parallel_tokens[:3]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 1\n",
    "According to the equations in the assignment instructions, if we want to update the translation probablity of a english word $e$ given a foreign word $f$, we have to count how often $e$ aligned with $f$ in the whole parallel corpus. Lets implement the function to count that in a pair of sentences.\n",
    "$$c(e|f;\\mathbf E, \\mathbf F) = \\frac{t(e|f)}{\\sum_{i=0}^{l_F}t(e|f_i)}\\sum_{j=1}^{l_E}\\delta(e, e_j)\\sum_{i=0}^{l_F}\\delta(f, f_i) \\\\\n",
    " \\delta(x, y) = \\begin{array}{lr}1,\\quad if \\quad x == y \\\\ 0, \\quad else \\end{array}$$\n",
    "Note that the word at index 0 of foreign sentence $\\mathbf F$ is NULL token.  \n",
    "In the next two steps, you will implement this count function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Compute the normalization term of the count function\n",
    "In the first step, the returned variable of your function, ```total_e```, should represent $\\sum_{i=0}^{l_F}t(e|f_i)$.  \n",
    "Note that ```total_e``` is the denominator of $c$.  \n",
    "Here is the example of the usage of it :  \n",
    "```total_e['apple']```$= \\sum_{i=0}^{l_F}t(apple|f_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalization_term(en_token_list: List[str], ch_token_list: List[str],\n",
    "                               t_ef: defaultdict) -> defaultdict:\n",
    "    total_e = defaultdict(lambda: 0)\n",
    "    #### YOUR CODE HERE ####\n",
    "    for i in list(set(en_token_list)):\n",
    "        for j in ch_token_list:\n",
    "            total_e[i] += t_ef[i][j]\n",
    "\n",
    "    return total_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('me', 0.6)\n",
      "('you', 0.6)\n",
      "('i', 0.6)\n",
      "('love', 0.6)\n",
      "('that', 0.6)\n"
     ]
    }
   ],
   "source": [
    "example_ch = ['[NULL]', '我', '愛', '你', '愛', '我']\n",
    "example_en = ['i', 'love', 'that', 'you', 'love', 'me']\n",
    "\n",
    "\n",
    "\"\"\" result\n",
    "('i', 0.6)\n",
    "('love', 0.6)\n",
    "('that', 0.6)\n",
    "('you', 0.6)\n",
    "('me', 0.6)\n",
    "\"\"\"\n",
    "for x in compute_normalization_term(example_en, example_ch,\n",
    "                                    defaultdict(lambda: defaultdict(lambda: 0.1))).items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Collect the count from the given bilingual sentences\n",
    "In the second step, the returned variable of your function, ```count_of_sent```, should represent the complete count function.  \n",
    "  Here is the example of the usage of it :  \n",
    "  ```count_of_sent['apple']['蘋果']``` $= c(apple|蘋果;\\mathbf E, \\mathbf F)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ef_count(en_token_list: List[str], ch_token_list: List[str],\n",
    "                     t_ef: defaultdict, total_e: defaultdict) -> defaultdict:\n",
    "    count_of_sent = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    #### YOUR CODE HERE ####\n",
    "    for en in en_token_list:\n",
    "        for ch in ch_token_list:\n",
    "            count_of_sent[en][ch] += t_ef[en][ch] / total_e[en]\n",
    "\n",
    "    return count_of_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[NULL]', '我', '愛', '你', '愛', '我'] ['i', 'love', 'that', 'you', 'love', 'me']\n",
      "\n",
      "('i', '[NULL]', 0.01)\n",
      "('i', '我', 0.02)\n",
      "('i', '愛', 0.02)\n",
      "('i', '你', 0.01)\n",
      "('love', '[NULL]', 0.02)\n",
      "('love', '我', 0.04)\n",
      "('love', '愛', 0.04)\n",
      "('love', '你', 0.02)\n",
      "('that', '[NULL]', 0.01)\n",
      "('that', '我', 0.02)\n",
      "('that', '愛', 0.02)\n",
      "('that', '你', 0.01)\n",
      "('you', '[NULL]', 0.01)\n",
      "('you', '我', 0.02)\n",
      "('you', '愛', 0.02)\n",
      "('you', '你', 0.01)\n",
      "('me', '[NULL]', 0.01)\n",
      "('me', '我', 0.02)\n",
      "('me', '愛', 0.02)\n",
      "('me', '你', 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(example_ch, example_en)\n",
    "print()\n",
    "\n",
    "\"\"\"result\n",
    "('i', '[NULL]', 0.01)\n",
    "('i', '我', 0.02)\n",
    "('i', '愛', 0.02)\n",
    "('i', '你', 0.01)\n",
    "('love', '[NULL]', 0.02)\n",
    "('love', '我', 0.04)\n",
    "('love', '愛', 0.04)\n",
    "('love', '你', 0.02)\n",
    "('that', '[NULL]', 0.01)\n",
    "('that', '我', 0.02)\n",
    "('that', '愛', 0.02)\n",
    "('that', '你', 0.01)\n",
    "('you', '[NULL]', 0.01)\n",
    "('you', '我', 0.02)\n",
    "('you', '愛', 0.02)\n",
    "('you', '你', 0.01)\n",
    "('me', '[NULL]', 0.01)\n",
    "('me', '我', 0.02)\n",
    "('me', '愛', 0.02)\n",
    "('me', '你', 0.01)\n",
    "\"\"\"\n",
    "for x, d in collect_ef_count(example_en, example_ch, \n",
    "                             defaultdict(lambda: defaultdict(lambda: 0.1)),\n",
    "                             defaultdict(lambda: 10.0)).items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Implement IBM Model 1\n",
    "Now you should be able to re-estimate the lexical probability distribution by\n",
    "$$t(e|f) = \\frac{\\sum_{(\\mathbf E, \\mathbf F)}c(e|f;\\mathbf E, \\mathbf F)}{\\sum_{e'}\\sum_{(\\mathbf e, \\mathbf f)}c(e'|f;\\mathbf E, \\mathbf F)}$$\n",
    "Note: You need to fill the code into where the comment strings start with '####'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model_1(parallel_corpus_list: List, n_iters: int) -> defaultdict:\n",
    "    en_vocab = set(en_token.lower() for en_token_list, _ in parallel_corpus_list\n",
    "                            for en_token in en_token_list)\n",
    "    ch_vocab = set(ch_token for _, ch_token_list in parallel_corpus_list\n",
    "                            for ch_token in ch_token_list)\n",
    "    print(f'length of en_vocab: {len(en_vocab)}')\n",
    "    print(f'lenght of ch_vocab: {len(ch_vocab)}')\n",
    "    \n",
    "    null_token = '[NULL]'\n",
    "    # Add the Null token\n",
    "    ch_vocab.add(null_token)\n",
    "    \n",
    "    # ptotal_edistribution is uniform.\n",
    "    init_prob = 1 / len(en_vocab)\n",
    "    # t_ef[e][f] means t(e|f)\n",
    "    t_ef = defaultdict(lambda: defaultdict(lambda: init_prob))\n",
    "    \n",
    "    \n",
    "    # EM process\n",
    "    for i in tqdm.tqdm(range(n_iters)):\n",
    "        # initialize\n",
    "        # total_ef_count[e][f] is the numerator ot new t(e|f)\n",
    "        total_ef_count = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        # total_f[f] is the Denominator ot new t(e|f)\n",
    "        total_f = defaultdict(lambda: 0.0) \n",
    "        token_pair_set = set()\n",
    "        perp = 0\n",
    "        # collect the evidence from corpus\n",
    "        for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "            ch_token_list = [null_token] + ch_token_list\n",
    "            perp += compute_perp(en_token_list, ch_token_list, t_ef)\n",
    "            \n",
    "            total_e = compute_normalization_term(en_token_list, ch_token_list, t_ef)\n",
    "            ef_count_of_sent = collect_ef_count(en_token_list, ch_token_list, t_ef, total_e)\n",
    "            \n",
    "            cur_token_pair_set = set((e, f) for e in en_token_list\n",
    "                                            for f in ch_token_list)\n",
    "            \n",
    "            for e, f in cur_token_pair_set:\n",
    "                #### You should write two lines of code here.  ####\n",
    "                #### Update total_ef_count and total_f with ef_count_of_sent ####\n",
    "                total_ef_count[e][f] += ef_count_of_sent[e][f]\n",
    "                total_f[f] += ef_count_of_sent[e][f]\n",
    "\n",
    "            token_pair_set.update(cur_token_pair_set)\n",
    "            \n",
    "        print(f'perplexity: {round(perp, 2)}')\n",
    "        # estimate probabilities\n",
    "        t_ef = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for e, f in token_pair_set:\n",
    "            #### You should write a line of code here. Update t_ef ####\n",
    "            t_ef[e][f] = total_ef_count[e][f] / total_f[f]\n",
    "            \n",
    "            \n",
    "    return t_ef\n",
    "\n",
    "def compute_perp(en_token_list, ch_token_list, t_ef):\n",
    "    l_e = len(en_token_list)\n",
    "    l_f = len(ch_token_list)\n",
    "    p_EF = 0.\n",
    "    for e in en_token_list:\n",
    "        s = 0.\n",
    "        for f in ch_token_list:\n",
    "            s += t_ef[e][f]\n",
    "        p_EF += -log(s)\n",
    "    p_EF += log(l_f**l_e)\n",
    "    return p_EF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "**You have to run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of en_vocab: 39570\n",
      "lenght of ch_vocab: 39788\n",
      "perplexity: 7488286.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:13<02:04, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 3158951.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:29<01:55, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2761088.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:44<01:42, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2633231.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:59<01:28, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2587444.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:14<01:13, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2567175.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:29<00:59, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2556695.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [01:45<00:45, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2550645.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [02:00<00:29, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2546848.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [02:15<00:15, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2544312.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:30<00:00, 15.07s/it]\n"
     ]
    }
   ],
   "source": [
    "t_ef = ibm_model_1(parallel_tokens, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "**You have to run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_prob(token_ch, translation_dict):\n",
    "    prob_dict = {}\n",
    "    for token_en, sub_dict in translation_dict.items():\n",
    "        if token_ch in sub_dict:\n",
    "            prob_dict[token_en] = sub_dict[token_ch]\n",
    "    return prob_dict\n",
    "\n",
    "def check_top_k_prob(token_ch, k, translation_dict):\n",
    "    prob_dict = get_conditional_prob(token_ch, translation_dict)\n",
    "    prob_list = list(prob_dict.items())\n",
    "    prob_sum = sum(prob_dict.values())\n",
    "    prob_list.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    if not isclose(prob_sum, 1.):\n",
    "        print(f\"Warning. sum of p( e | {token_ch}) = {prob_sum}, not close to 1.\")\n",
    "    \n",
    "    for token_en, prob in prob_list[:k]:\n",
    "        print(f\"p({token_en} | {token_ch}) = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(school | 學校) = 0.7543461511392134\n",
      "p(schools | 學校) = 0.14944245769188017\n",
      "p(at | 學校) = 0.08704250211023351\n",
      "p(the | 學校) = 0.003338099172844307\n",
      "p(has | 學校) = 0.00237057305392511\n"
     ]
    }
   ],
   "source": [
    "\"\"\" results for reference\n",
    "p(school | 學校) = 0.7592438820372703\n",
    "p(schools | 學校) = 0.152040212014728\n",
    "p(at | 學校) = 0.08116603608007476\n",
    "p(the | 學校) = 0.0024247348456308397\n",
    "p(has | 學校) = 0.0024026143118947804\n",
    "\"\"\"\n",
    "\n",
    "check_top_k_prob('學校', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(businesses | 企業) = 0.46790246504794336\n",
      "p(business | 企業) = 0.37723839585937424\n",
      "p(by | 企業) = 0.0327445724463648\n",
      "p(run | 企業) = 0.02767444805585238\n",
      "p(of | 企業) = 0.024229859533618347\n"
     ]
    }
   ],
   "source": [
    "\"\"\" results for reference\n",
    "p(businesses | 企業) = 0.46758578752806945\n",
    "p(business | 企業) = 0.3901655681518314\n",
    "p(by | 企業) = 0.03300134949576897\n",
    "p(run | 企業) = 0.02862452153472591\n",
    "p(a | 企業) = 0.019347178868170722\n",
    "\"\"\"\n",
    "\n",
    "check_top_k_prob('企業', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(budget | 預算) = 0.6651040322779282\n",
      "p(are | 預算) = 0.1120630044048272\n",
      "p(within | 預算) = 0.041317828391582066\n",
      "p(already | 預算) = 0.036872539743076756\n",
      "p(going | 預算) = 0.025828859426797895\n"
     ]
    }
   ],
   "source": [
    "\"\"\" results for reference\n",
    "p(budget | 預算) = 0.675000585629638\n",
    "p(are | 預算) = 0.1145673270578127\n",
    "p(within | 預算) = 0.03939258531542436\n",
    "p(already | 預算) = 0.03737763058988209\n",
    "p(overspent | 預算) = 0.024970159365872654\n",
    "\"\"\"\n",
    "\n",
    "check_top_k_prob('預算', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(century | 世紀) = 0.520018781606513\n",
      "p(centuries | 世紀) = 0.2046575675065016\n",
      "p(last | 世紀) = 0.11143708312156175\n",
      "p(than | 世紀) = 0.04009121603820709\n",
      "p(more | 世紀) = 0.01659082208644732\n"
     ]
    }
   ],
   "source": [
    "check_top_k_prob('世紀', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(textile | 紡織) = 0.2729531600155855\n",
      "p(consortium | 紡織) = 0.13647791958731112\n",
      "p(manufacturers | 紡織) = 0.13647791958731112\n",
      "p(makes | 紡織) = 0.12974917399130032\n",
      "p(selling | 紡織) = 0.12974850934984983\n"
     ]
    }
   ],
   "source": [
    "check_top_k_prob('紡織', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(headteacher | 校長) = 0.2505243011545617\n",
      "p(principal | 校長) = 0.1262554506553137\n",
      "p(head | 校長) = 0.08072821755931443\n",
      "p(headmaster | 校長) = 0.06524380232411475\n",
      "p(were | 校長) = 0.05229879235203732\n"
     ]
    }
   ],
   "source": [
    "check_top_k_prob('校長', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(us | 美國) = 0.39744321919826436\n",
      "p(american | 美國) = 0.22779138145551914\n",
      "p(the | 美國) = 0.16408318429241966\n",
      "p(states | 美國) = 0.09007608903992324\n",
      "p(in | 美國) = 0.05922861682833806\n"
     ]
    }
   ],
   "source": [
    "check_top_k_prob('美國', 5, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(police | 員警) = 0.7285996042387128\n",
      "p(officer | 員警) = 0.10771540774260735\n",
      "p(officers | 員警) = 0.07037183572477326\n",
      "p(the | 員警) = 0.04652981740933166\n",
      "p(policeman | 員警) = 0.03648325539859977\n"
     ]
    }
   ],
   "source": [
    "check_top_k_prob('員警', 5, t_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 2\n",
    "Recall that the count functions of lexical translation and alignment in the assignment instructions:\n",
    "$$\\begin{eqnarray}\n",
    "c_1(e|f;\\mathbf E, \\mathbf F) \n",
    "&=&\\sum_{j=1}^{l_E}\\sum_{i=0}^{l_F}\n",
    "\\frac{t(e|f)a(i|j,l_E,l_F )\\delta(e, e_j)\\delta(f, f_i)}\n",
    "{\\sum_{i'=0}^{l_F}t(e|f_{i'})a(i'|j,l_E,l_F )}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "c_2(i|j, l_E, l_F;\\mathbf E, \\mathbf F) &=& \n",
    "\\frac{t(e_j|f_i) a(i|j,l_E,l_F)}{\\sum_{i'=0}^{l_F}t(e_j|f_{i'})a(i'|j,l_E,l_F )}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Similarly, you will implement the count functions in two steps. In the final step, you would be asked to complete the implementation of IBM Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Compute the normalization term of the count function\n",
    "```total_e[english_word][j]``` is a defaultdict with two keys, and it is actually the count function of alignment.  \n",
    "You may be aware to the fact that $\\sum_{j}$```total_e[english_word][j]``` is the count function of lexical translation if you note the difference between the normalization terms of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alg_normalization_term(en_token_list: List[str], ch_token_list: List[str],\n",
    "                                   t_ef: defaultdict, q_alg: defaultdict) -> defaultdict:\n",
    "    total_e = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    l_e = len(en_token_list)\n",
    "    # the lenth of foreign sentence does not take NULL token into account\n",
    "    l_f = len(ch_token_list) - 1\n",
    "    \n",
    "    for j in range(1, l_e + 1):\n",
    "        en_word = en_token_list[j - 1]\n",
    "        for i in range(l_f + 1):\n",
    "            ch_word = ch_token_list[i]\n",
    "            #### YOUR CODE HERE ####\n",
    "            total_e[en_word][j] += t_ef[en_word][ch_word] * q_alg[i][j][l_e][l_f]\n",
    "\n",
    "    return total_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[NULL]', '我', '愛', '你', '愛', '我'] ['i', 'love', 'that', 'you', 'love', 'me']\n",
      "\n",
      "('i', 1, 0.12000000000000002)\n",
      "('love', 2, 0.12000000000000002)\n",
      "('love', 5, 0.12000000000000002)\n",
      "('that', 3, 0.12000000000000002)\n",
      "('you', 4, 0.12000000000000002)\n",
      "('me', 6, 0.12000000000000002)\n"
     ]
    }
   ],
   "source": [
    "print(example_ch, example_en)\n",
    "print()\n",
    "\n",
    "\"\"\" result\n",
    "('i', 1, 0.12000000000000002)\n",
    "('love', 2, 0.12000000000000002)\n",
    "('love', 5, 0.12000000000000002)\n",
    "('that', 3, 0.12000000000000002)\n",
    "('you', 4, 0.12000000000000002)\n",
    "('me', 6, 0.12000000000000002)\n",
    "\"\"\"\n",
    "for x, d in compute_alg_normalization_term(example_en, example_ch,\n",
    "                                           defaultdict(lambda: defaultdict(lambda: 0.1)),\n",
    "                                           defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2))))\n",
    "                                          ).items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Collect the count for translation and the count for alignment from the given bilingual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_count(en_token_list: List[str], ch_token_list: List[str],\n",
    "                    t_ef: defaultdict, q_alg: defaultdict,\n",
    "                    total_e: defaultdict) -> defaultdict:\n",
    "    ef_count = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    alg_count = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "    l_e = len(en_token_list)\n",
    "    # the lenth of foreign sentence does not take NULL token into account\n",
    "    l_f = len(ch_token_list) - 1\n",
    "    \n",
    "    for j in range(1, l_e + 1):\n",
    "        en_word = en_token_list[j - 1]\n",
    "        for i in range(l_f + 1):\n",
    "            ch_word = ch_token_list[i]\n",
    "            #### YOUR CODE HERE ####\n",
    "            ef_count[en_word][ch_word] += t_ef[en_word][ch_word] * q_alg[i][j][l_e][l_f] / total_e[en_word][j]\n",
    "            alg_count[i][j][l_e][l_f] += t_ef[en_word][ch_word] * q_alg[i][j][l_e][l_f] / total_e[en_word][j]\n",
    "            \n",
    "    return (ef_count, alg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[NULL]', '我', '愛', '你', '愛', '我'] ['i', 'love', 'that', 'you', 'love', 'me']\n",
      "\n",
      "('i', '[NULL]', 0.20000000000000004)\n",
      "('i', '我', 0.4000000000000001)\n",
      "('i', '愛', 0.4000000000000001)\n",
      "('i', '你', 0.20000000000000004)\n",
      "('love', '[NULL]', 0.4000000000000001)\n",
      "('love', '我', 0.8000000000000002)\n",
      "('love', '愛', 0.8000000000000002)\n",
      "('love', '你', 0.4000000000000001)\n",
      "('that', '[NULL]', 0.20000000000000004)\n",
      "('that', '我', 0.4000000000000001)\n",
      "('that', '愛', 0.4000000000000001)\n",
      "('that', '你', 0.20000000000000004)\n",
      "('you', '[NULL]', 0.20000000000000004)\n",
      "('you', '我', 0.4000000000000001)\n",
      "('you', '愛', 0.4000000000000001)\n",
      "('you', '你', 0.20000000000000004)\n",
      "('me', '[NULL]', 0.20000000000000004)\n",
      "('me', '我', 0.4000000000000001)\n",
      "('me', '愛', 0.4000000000000001)\n",
      "('me', '你', 0.20000000000000004)\n",
      "\n",
      "(0, 1, 0.20000000000000004)\n",
      "(0, 2, 0.20000000000000004)\n",
      "(0, 3, 0.20000000000000004)\n",
      "(0, 4, 0.20000000000000004)\n",
      "(0, 5, 0.20000000000000004)\n",
      "(0, 6, 0.20000000000000004)\n",
      "(1, 1, 0.20000000000000004)\n",
      "(1, 2, 0.20000000000000004)\n",
      "(1, 3, 0.20000000000000004)\n",
      "(1, 4, 0.20000000000000004)\n",
      "(1, 5, 0.20000000000000004)\n",
      "(1, 6, 0.20000000000000004)\n",
      "(2, 1, 0.20000000000000004)\n",
      "(2, 2, 0.20000000000000004)\n",
      "(2, 3, 0.20000000000000004)\n",
      "(2, 4, 0.20000000000000004)\n",
      "(2, 5, 0.20000000000000004)\n",
      "(2, 6, 0.20000000000000004)\n",
      "(3, 1, 0.20000000000000004)\n",
      "(3, 2, 0.20000000000000004)\n",
      "(3, 3, 0.20000000000000004)\n",
      "(3, 4, 0.20000000000000004)\n",
      "(3, 5, 0.20000000000000004)\n",
      "(3, 6, 0.20000000000000004)\n",
      "(4, 1, 0.20000000000000004)\n",
      "(4, 2, 0.20000000000000004)\n",
      "(4, 3, 0.20000000000000004)\n",
      "(4, 4, 0.20000000000000004)\n",
      "(4, 5, 0.20000000000000004)\n",
      "(4, 6, 0.20000000000000004)\n",
      "(5, 1, 0.20000000000000004)\n",
      "(5, 2, 0.20000000000000004)\n",
      "(5, 3, 0.20000000000000004)\n",
      "(5, 4, 0.20000000000000004)\n",
      "(5, 5, 0.20000000000000004)\n",
      "(5, 6, 0.20000000000000004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_ch, example_en)\n",
    "print()\n",
    "\n",
    "\"\"\"result\n",
    "('i', '[NULL]', 0.20000000000000004)\n",
    "('i', '我', 0.4000000000000001)\n",
    "('i', '愛', 0.4000000000000001)\n",
    "('i', '你', 0.20000000000000004)\n",
    "('love', '[NULL]', 0.4000000000000001)\n",
    "('love', '我', 0.8000000000000002)\n",
    "('love', '愛', 0.8000000000000002)\n",
    "('love', '你', 0.4000000000000001)\n",
    "('that', '[NULL]', 0.20000000000000004)\n",
    "('that', '我', 0.4000000000000001)\n",
    "('that', '愛', 0.4000000000000001)\n",
    "('that', '你', 0.20000000000000004)\n",
    "('you', '[NULL]', 0.20000000000000004)\n",
    "('you', '我', 0.4000000000000001)\n",
    "('you', '愛', 0.4000000000000001)\n",
    "('you', '你', 0.20000000000000004)\n",
    "('me', '[NULL]', 0.20000000000000004)\n",
    "('me', '我', 0.4000000000000001)\n",
    "('me', '愛', 0.4000000000000001)\n",
    "('me', '你', 0.20000000000000004)\n",
    "\n",
    "(0, 1, 0.20000000000000004)\n",
    "...\n",
    "(5, 5, 0.20000000000000004)\n",
    "(5, 6, 0.20000000000000004)\n",
    "\"\"\"\n",
    "example_ef, example_alg = collect_count(example_en, example_ch,\n",
    "                                        defaultdict(lambda: defaultdict(lambda: 0.1)),\n",
    "                                        defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2)))),\n",
    "                                        defaultdict(lambda: defaultdict(lambda: 0.1))\n",
    "                                       )\n",
    "for x, d in example_ef.items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z))\n",
    "print()\n",
    "\n",
    "for x, d in example_alg.items():\n",
    "    for y, z in d.items():\n",
    "        print((x, y, z[len(example_en)][len(example_ch)-1]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Implement IBM Model 2\n",
    "Now you should be able to re-estimate the probability distribution of lexical translation and that of alignment by  \n",
    "$$t(e|f) = \\frac{\\sum_{(\\mathbf E, \\mathbf F)}c_1(e|f;\\mathbf E, \\mathbf F)}{\\sum_{e'}\\sum_{(\\mathbf E, \\mathbf F)}c_1(e'|f;\\mathbf E, \\mathbf F)}$$\n",
    "\n",
    "$$a(i|j, l_E, l_F) = \\frac{\\sum_{(\\mathbf E, \\mathbf F)}c_2(i|j, l_E, l_F;\\mathbf E, \\mathbf F)}{\\sum_{i'}\\sum_{(\\mathbf E, \\mathbf F)}c_2(i'|j, l_E, l_F;\\mathbf E, \\mathbf F)}$$\n",
    "\n",
    "Note: You need to fill the code into where the comment strings start with '####'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model_2(parallel_corpus_list: List, n_iters: int,\n",
    "                init_t_ef: defaultdict) -> Tuple[defaultdict, defaultdict]:\n",
    "    en_vocab = set(en_token.lower() for en_token_list, _ in parallel_corpus_list\n",
    "                            for en_token in en_token_list)\n",
    "    ch_vocab = set(ch_token for _, ch_token_list in parallel_corpus_list\n",
    "                            for ch_token in ch_token_list)\n",
    "    print(f'length of en_vocab: {len(en_vocab)}')\n",
    "    print(f'lenght of ch_vocab: {len(ch_vocab)}')\n",
    "    null_token = '[NULL]'\n",
    "    # Add the Null token\n",
    "    ch_vocab.add(null_token)\n",
    "    \n",
    "    t_ef = deepcopy(init_t_ef)\n",
    "    q_alg = get_initial_q_alg(parallel_corpus_list) # q_alg[i][j][l_e][l_f] means a(i | j, l_e, l_f)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(n_iters)):\n",
    "        total_ef_count = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        total_f = defaultdict(lambda: 0.0)\n",
    "\n",
    "        # total_alg_count[i][j][l_e][l_f] is the numerator of new a(i | j, l_e, l_f)\n",
    "        total_alg_count = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "        # total_alg_count[j][l_e][l_f] is the denominator of new a(i | j, l_e, l_f)\n",
    "        total_alg = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n",
    "\n",
    "        token_pair_set = set()\n",
    "        perp = 0\n",
    "        # collect the evidence from corpus\n",
    "        for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "            ch_token_list = [null_token] + ch_token_list\n",
    "            perp += compute_perp(en_token_list, ch_token_list, t_ef, q_alg)\n",
    "            # the lenth of foreign sentence does not take NULL token into account\n",
    "            l_f = len(ch_token_list) - 1\n",
    "            l_e = len(en_token_list)\n",
    "\n",
    "            total_e = compute_alg_normalization_term(en_token_list, ch_token_list, t_ef, q_alg)\n",
    "            ef_count_of_sent, alg_count_of_sent = collect_count(en_token_list, ch_token_list,\n",
    "                                                                t_ef, q_alg, total_e)\n",
    "            \n",
    "            cur_token_pair_set = set((e, f) for e in en_token_list\n",
    "                                            for f in ch_token_list)\n",
    "            token_pair_set.update(cur_token_pair_set)\n",
    "            for e, f in cur_token_pair_set:\n",
    "                #### You should write two lines of code here.  ####\n",
    "                #### Update total_ef_count and total_f with ef_count_of_sent####\n",
    "                total_ef_count[e][f] += ef_count_of_sent[e][f]\n",
    "                total_f[f] += ef_count_of_sent[e][f]\n",
    "                \n",
    "                \n",
    "            # collect counts\n",
    "            for j in range(1, l_e+1):\n",
    "                for i in range(l_f+1):\n",
    "                    #### You should write two lines of code here.  ####\n",
    "                    #### Update total_alg_count, total_alg with alg_count_of_sent ####\n",
    "                    total_alg_count[i][j][l_e][l_f] += alg_count_of_sent[i][j][l_e][l_f]\n",
    "                    total_alg[j][l_e][l_f] += alg_count_of_sent[i][j][l_e][l_f]\n",
    "                    \n",
    "                    \n",
    "        print(f'perplexity: {round(perp, 2)}')\n",
    "        \n",
    "        # Estimate the new lexical translation probabilities\n",
    "        t_ef = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for e, f in token_pair_set:\n",
    "            #### You should write a line of code here. Update t_ef ####\n",
    "            t_ef[e][f] = total_ef_count[e][f] / total_f[f]\n",
    "            \n",
    "            \n",
    "        # Estimate the new alignment probabilities\n",
    "        q_alg = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "        for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "            l_f = len(ch_token_list)\n",
    "            l_e = len(en_token_list)\n",
    "            for i in range(l_f + 1):\n",
    "                for j in range(1, l_e + 1):\n",
    "                    #### You should write a line of code here. Update q_alg ####\n",
    "                    q_alg[i][j][l_e][l_f] = total_alg_count[i][j][l_e][l_f] / total_alg[j][l_e][l_f]\n",
    "                    \n",
    "    return (t_ef, q_alg)\n",
    "\n",
    "def get_initial_q_alg(parallel_corpus_list: List) -> defaultdict:\n",
    "    q_alg = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.0))))\n",
    "    \n",
    "    for en_token_list, ch_token_list in parallel_corpus_list:\n",
    "        l_e = len(en_token_list)\n",
    "        l_f = len(ch_token_list)\n",
    "        q_initial = 1 / (l_f + 1)\n",
    "        for i in range(0, l_f + 1): # 0 for NULL token\n",
    "            for j in range(1, l_e + 1):\n",
    "                q_alg[i][j][l_e][l_f] = q_initial\n",
    "    return q_alg\n",
    "\n",
    "def compute_perp(en_token_list, ch_token_list, t_ef, q_alg):\n",
    "    perp = 0\n",
    "    l_e = len(en_token_list)\n",
    "    l_f = len(ch_token_list) - 1\n",
    "    for j in range(1, l_e+1):\n",
    "        en_word = en_token_list[j - 1]\n",
    "        cur_perp = 0\n",
    "        for i in range(l_f + 1):\n",
    "            ch_word = ch_token_list[i]\n",
    "            cur_perp += t_ef[en_word][ch_word] * q_alg[i][j][l_e][l_f]\n",
    "        perp += -log(cur_perp)\n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "**You have to run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of en_vocab: 39570\n",
      "lenght of ch_vocab: 39788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2542534.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/15 [00:47<11:09, 47.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2292109.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 2/15 [01:36<10:24, 48.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2204236.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 3/15 [02:24<09:37, 48.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2170161.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 4/15 [03:13<08:52, 48.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2156160.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 5/15 [04:02<08:06, 48.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2146954.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 6/15 [04:51<07:18, 48.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2139940.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 7/15 [05:40<06:30, 48.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2134330.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 8/15 [06:29<05:41, 48.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2129798.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 9/15 [07:17<04:51, 48.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2126041.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 10/15 [08:06<04:03, 48.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2122927.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 11/15 [08:55<03:14, 48.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2120324.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 12/15 [09:43<02:25, 48.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2118097.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 13/15 [10:32<01:37, 48.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2116201.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 14/15 [11:20<00:48, 48.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 2114541.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [12:09<00:00, 48.61s/it]\n"
     ]
    }
   ],
   "source": [
    "t_ef_2, q_alg = ibm_model_2(parallel_tokens, 15, t_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_prob(token_ch, k, translation_dict):\n",
    "    prob_dict = get_conditional_prob(token_ch, translation_dict)\n",
    "    prob_list = list(prob_dict.items())\n",
    "    prob_sum = sum(prob_dict.values())\n",
    "    prob_list.sort(key=lambda x:x[1], reverse=True)\n",
    "    if not isclose(prob_sum, 1.):\n",
    "        print(f\"Warning. sum of p( e | {token_ch}) = {prob_sum}, not close to 1.\")\n",
    "    \n",
    "    return prob_list[:k]\n",
    "\n",
    "def translate(ch_token_list: List[str], l_e: int, translation_dict, align):\n",
    "    l_f = len(ch_token_list)\n",
    "    null_token = '[NULL]'\n",
    "    ch_token_list.insert(0, null_token)\n",
    "    k = 3\n",
    "    \n",
    "    en_candidates = []\n",
    "    for token_ch in ch_token_list:\n",
    "        en_candidates.append(get_top_k_prob(token_ch, 10, translation_dict))\n",
    "\n",
    "    for j in range(1, l_e+1):\n",
    "        possible_list = []\n",
    "        for i, ens in enumerate(en_candidates):\n",
    "            for token_en, prob in ens:\n",
    "                possible_list.append((token_en, align[i][j][l_e][l_f] * prob))\n",
    "        possible_list.sort(key=lambda x:x[1], reverse=True)\n",
    "        print(f\"possible candidates for the {j}-th English word:\")\n",
    "        for token_en, prob in possible_list[:k]:\n",
    "            print(f\"{token_en}\\t\\tprobability:{prob}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible candidates for the 1-th English word:\n",
      "i\t\tprobability:0.3127890726046207\n",
      "a\t\tprobability:0.07578017037116472\n",
      "my\t\tprobability:0.07443197844766618\n",
      "\n",
      "possible candidates for the 2-th English word:\n",
      "i\t\tprobability:0.22366274850534007\n",
      "bike\t\tprobability:0.17055958090381096\n",
      "bicycle\t\tprobability:0.07670599220180266\n",
      "\n",
      "possible candidates for the 3-th English word:\n",
      "riding\t\tprobability:0.08944179294600363\n",
      "bike\t\tprobability:0.0879748255140529\n",
      "will\t\tprobability:0.07686275875064903\n",
      "\n",
      "possible candidates for the 4-th English word:\n",
      "i\t\tprobability:0.15569392796224918\n",
      "riding\t\tprobability:0.08984803037656637\n",
      "ride\t\tprobability:0.07676970665583598\n",
      "\n",
      "possible candidates for the 5-th English word:\n",
      "bike\t\tprobability:0.34370933145383814\n",
      "bicycle\t\tprobability:0.15457686492002765\n",
      "i\t\tprobability:0.08895165883739573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" result\n",
    "possible candidates for the 1-th word:\n",
    "i\t\tprobability:0.3051610136893576\n",
    "a\t\tprobability:0.08537920692347092\n",
    "(\t\tprobability:0.07314717156010928\n",
    "\n",
    "possible candidates for the 2-th word:\n",
    "i\t\tprobability:0.2220627843510443\n",
    "bike\t\tprobability:0.1721509922781537\n",
    "bicycle\t\tprobability:0.07748032610499858\n",
    "\n",
    "possible candidates for the 3-th word:\n",
    "riding\t\tprobability:0.08936528673455782\n",
    "bike\t\tprobability:0.08840401330382662\n",
    "will\t\tprobability:0.07680236346367117\n",
    "\n",
    "possible candidates for the 4-th word:\n",
    "i\t\tprobability:0.15463795029280586\n",
    "riding\t\tprobability:0.08968372901915127\n",
    "will\t\tprobability:0.07672989741828276\n",
    "\n",
    "possible candidates for the 5-th word:\n",
    "bike\t\tprobability:0.3455995109696529\n",
    "bicycle\t\tprobability:0.15554463240264935\n",
    "i\t\tprobability:0.08811550005889636\n",
    "\"\"\"\n",
    "translate(['我', '會', '騎', '腳踏車'], 5, t_ef_2, q_alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible candidates for the 1-th English word:\n",
      "today\t\tprobability:0.21220751845875455\n",
      "this\t\tprobability:0.1124239475689944\n",
      "a\t\tprobability:0.07578017037116472\n",
      "\n",
      "possible candidates for the 2-th English word:\n",
      "weather\t\tprobability:0.1693760489931085\n",
      "today\t\tprobability:0.15174096855991503\n",
      "good\t\tprobability:0.09825486201637977\n",
      "\n",
      "possible candidates for the 3-th English word:\n",
      "weather\t\tprobability:0.23787023699745397\n",
      "very\t\tprobability:0.1223009252029452\n",
      "'s\t\tprobability:0.07301316608322325\n",
      "\n",
      "possible candidates for the 4-th English word:\n",
      "weather\t\tprobability:0.23689158750217237\n",
      "very\t\tprobability:0.12285640616965483\n",
      "today\t\tprobability:0.10562844097091682\n",
      "\n",
      "possible candidates for the 5-th English word:\n",
      "good\t\tprobability:0.19800185223710542\n",
      "a\t\tprobability:0.08225636464158124\n",
      "weather\t\tprobability:0.07570919589083555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(['今天','天氣','很','好'], 5, t_ef_2, q_alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
